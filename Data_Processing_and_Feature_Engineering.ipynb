{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "**Purpose**: This notebook imports raw market data from Databento, processing order book and trade records, and engineering features for analysis. It includes functions for filtering, merging, and aggregating the raw data to create a clean dataset with state-level and order-level features. Finally, it generates summary statistics and prepares train, validation, and test datasets for subsequent modeling."
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"0naVr3hZBN7btW9qqHRQj4",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import databento as db\n",
    "import datetime\n",
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pytz\n",
    "import requests\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "import warnings\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "import cupy as cp\n",
    "import cudf\n",
    "from cuml.preprocessing import StandardScaler\n",
    "from cuml.linear_model import Ridge, ElasticNet\n",
    "from cuml.ensemble import RandomForestRegressor as cuRF\n",
    "from lightgbm import LGBMRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "execution_count":4,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"H3VWu9TB8H6vBew8WM8OkZ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Prepare the Orderbook Data"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"zzMWtWNBH3wzdlKfPH2aK3",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def import_raw_mbp_data(ticker, year, month, day, exchange):\n",
    "    \"\"\"\n",
    "    Imports mbp-10 data for a given day and ticker\n",
    "    \"\"\"\n",
    "    client = db.Historical(#API_KEY)\n",
    "    if exchange == 'xnas':\n",
    "        try:\n",
    "            df = db.DBNStore.from_file(f'\/data\/workspace_files\/mbp-10\/{ticker}\/xnas-itch-{year}{month}{day}.mbp-10.{ticker}.dbn.zst').to_df()\n",
    "        except:\n",
    "            data = client.timeseries.get_range(\n",
    "                    dataset=\"XNAS.ITCH\",\n",
    "                    schema=\"mbp-10\",\n",
    "                    stype_in=\"raw_symbol\",\n",
    "                    symbols=[ticker],\n",
    "                    start=f\"{year}-{month}-{day}T08:00:00\",\n",
    "                    end=f\"{year}-{month}-{day}T24:00:00\",\n",
    "                )\n",
    "            df = data.to_df()\n",
    "    else:\n",
    "        try:\n",
    "            df = db.DBNStore.from_file(f'\/data\/workspace_files\/mbp-10\/{ticker}\/xnys-pillar-{year}{month}{day}.mbp-10.{ticker}.dbn.zst').to_df()\n",
    "        except:\n",
    "            data = client.timeseries.get_range(\n",
    "                    dataset=\"XNAS.ITCH\",\n",
    "                    schema=\"mbp-10\",\n",
    "                    stype_in=\"raw_symbol\",\n",
    "                    symbols=[ticker],\n",
    "                    start=f\"{year}-{month}-{day}T08:00:00\",\n",
    "                    end=f\"{year}-{month}-{day}T24:00:00\",\n",
    "                )\n",
    "            df = data.to_df()\n",
    "            \n",
    "    df['ts_event_local'] = pd.to_datetime(df['ts_event'], utc=True).dt.tz_convert('America\/New_York') # convert time-zone\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.set_index('ts_event_local', drop=False)                                                                                       \n",
    "    df = df[(df['ts_event_local'].dt.time >= datetime.time(9, 40)) & \n",
    "            (df['ts_event_local'].dt.time <= datetime.time(15, 50))\n",
    "        ]\n",
    "    df['ts_event'] = df['ts_event_local']\n",
    "    df = df.drop(columns=['rtype', 'publisher_id', 'instrument_id', 'action', 'side',\n",
    "                        'depth', 'price', 'size', 'flags', 'ts_in_delta', 'sequence',\n",
    "                        'symbol', 'ts_event_local'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def import_raw_mbo_data(ticker, year, month, day, exchange):\n",
    "    \"\"\"\n",
    "    Imports mbo data for a given day and ticker\n",
    "    \"\"\"\n",
    "    client = db.Historical(API_KEY)\n",
    "    if exchange == 'xnas':\n",
    "        try:\n",
    "            df = db.DBNStore.from_file(f'\/data\/workspace_files\/mbo\/{ticker}\/xnas-itch-{year}{month}{day}.mbo.{ticker}.dbn.zst').to_df()\n",
    "        except:\n",
    "            data = client.timeseries.get_range(\n",
    "                    dataset=\"XNAS.ITCH\",\n",
    "                    schema=\"mbo\",\n",
    "                    stype_in=\"raw_symbol\",\n",
    "                    symbols=[ticker],\n",
    "                    start=f\"{year}-{month}-{day}T08:00:00\",\n",
    "                    end=f\"{year}-{month}-{day}T24:00:00\",\n",
    "                )\n",
    "            df = data.to_df()\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            df = db.DBNStore.from_file(f'\/data\/workspace_files\/mbo\/{ticker}\/xnys-pillar-{year}{month}{day}.mbo.{ticker}.dbn.zst').to_df()\n",
    "        except:\n",
    "            data = client.timeseries.get_range(\n",
    "                    dataset=\"XNYS.PILLAR\",\n",
    "                    schema=\"mbo\",\n",
    "                    stype_in=\"raw_symbol\",\n",
    "                    symbols=[ticker],\n",
    "                    start=f\"{year}-{month}-{day}T08:00:00\",\n",
    "                    end=f\"{year}-{month}-{day}T24:00:00\",\n",
    "                )\n",
    "            df = data.to_df()\n",
    "\n",
    "    df['ts_event_local'] = pd.to_datetime(df['ts_event'], utc=True).dt.tz_convert('America\/New_York') # convert time-zone\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = df.set_index('ts_event_local', drop=False)                                                                                       \n",
    "    df = df[(df['ts_event_local'].dt.time >= datetime.time(9, 40)) & \n",
    "            (df['ts_event_local'].dt.time <= datetime.time(15, 50))\n",
    "        ]\n",
    "    df['ts_event'] = df['ts_event_local']\n",
    "    df = df.drop(columns=['rtype', 'publisher_id', 'instrument_id', 'channel_id',\n",
    "                            'flags', 'ts_in_delta', 'sequence', 'ts_event_local'])\n",
    "\n",
    "    return df"
   ],
   "execution_count":14,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"p16UrydFoyowPWR3tYeSYn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def filter_order_actions(df):\n",
    "    \"\"\"\n",
    "    Given a DataFrame with columns 'ts_event', 'order_id', and 'action',\n",
    "    drop rows where action is 'F' or 'C' at any timestamp (ts_event) where a 'T' action also appears.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing 'ts_event', 'order_id', and 'action' columns.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with the undesired rows dropped.\n",
    "    \"\"\"\n",
    "    # Identify ts_event timestamps where a 'T' action appears.\n",
    "    ts_events_with_T = df.loc[df['action'] == 'T', 'ts_event'].unique()\n",
    "    \n",
    "    # Create a mask to drop rows with actions 'F' or 'C' at those ts_events.\n",
    "    mask = ~(df['ts_event'].isin(ts_events_with_T) & df['action'].isin(['F', 'C']))\n",
    "    \n",
    "    # Return the filtered DataFrame.\n",
    "    return df[mask].copy()\n",
    "\n",
    "\n",
    "def combine_T_rows(df):\n",
    "    \"\"\"\n",
    "    Some trades occur at the exact same timestamp with the same side. Treat these as one trade.\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Modified DataFrame with combined 'T' rows, sorted by ts_event.\n",
    "    \"\"\"\n",
    "    # Separate rows with action 'T' from those with other actions\n",
    "    df_T = df[df['action'] == 'T'].copy()\n",
    "    df_non_T = df[df['action'] != 'T'].copy()\n",
    "    \n",
    "    # Function to combine rows in each group\n",
    "    def combine_group(group):\n",
    "        total_size = group['size'].sum()\n",
    "        weighted_price = np.average(group['price'], weights=group['size'])\n",
    "        # Use the first row as a template for the group\n",
    "        combined = group.iloc[0].copy()\n",
    "        combined['price'] = weighted_price\n",
    "        combined['size'] = total_size\n",
    "        return combined\n",
    "\n",
    "    # Group by order_id, ts_event, and side, then combine rows within each group\n",
    "    df_T_combined = (\n",
    "        df_T.groupby(['order_id', 'ts_event', 'side'], as_index=False)\n",
    "            .apply(combine_group)\n",
    "            .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Concatenate the combined 'T' rows with the non-'T' rows\n",
    "    df_result = pd.concat([df_T_combined, df_non_T], ignore_index=True)\n",
    "    \n",
    "    # Resort the merged DataFrame by the ts_event column\n",
    "    df_result = df_result.sort_values('ts_event').reset_index(drop=True)\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "\n",
    "def create_p_plus_minus(ob, mbp):\n",
    "    \"\"\"\n",
    "    Merge the 'ob' dataframe with the 'mbp' dataframe based on the 'ts_event' column.\n",
    "    For each row in 'ob', the corresponding minus row from 'mbp' is the one with the greatest\n",
    "    'ts_event' that is strictly less than the 'ts_event' in 'ob'. The corresponding plus row \n",
    "    from 'mbp' is the one with the smallest 'ts_event' that is strictly greater than the 'ts_event' \n",
    "    in 'ob'. \n",
    "\n",
    "    Parameters:\n",
    "        ob (pd.DataFrame): The orderbook dataframe with a 'ts_event' column.\n",
    "        mbp (pd.DataFrame): The mbp dataframe with a 'ts_event' column.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The merged dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    ob = ob.sort_values('ts_event')\n",
    "    mbp = mbp.sort_values('ts_event')\n",
    "\n",
    "    # Append the suffix 'minus' to all column names in 'mbp' except 'ts_event'\n",
    "    mbp_renamed = mbp.rename(columns={col: f\"{col}_minus\" for col in mbp.columns if col != 'ts_event'})\n",
    "\n",
    "    # Perform the merge-asof operation\n",
    "    merged_df = pd.merge_asof(\n",
    "        ob, \n",
    "        mbp_renamed, \n",
    "        on='ts_event', \n",
    "        direction='backward',  # Ensures the greatest time in 'mbp' that is strictly less than 'trades'\n",
    "        allow_exact_matches=False\n",
    "    )\n",
    "\n",
    "    # Append the suffix 'minus' to all column names in 'mbp' except 'ts_event'\n",
    "    mbp_renamed = mbp.rename(columns={col: f\"{col}_plus\" for col in mbp.columns if col != 'ts_event'})\n",
    "\n",
    "    # Perform the merge-asof operation\n",
    "    merged_df = pd.merge_asof(\n",
    "        merged_df, \n",
    "        mbp_renamed, \n",
    "        on='ts_event', \n",
    "        direction='forward',  # Ensures the greatest time in 'mbp' that is strictly less than 'trades'\n",
    "        allow_exact_matches=True\n",
    "    )\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Lee & Ready algo\n",
    "def assign_side(df):\n",
    "    def apply_tick_test(df, index):\n",
    "        if index == 0:\n",
    "            return 'B'  # Default to 'B' if no previous trade exists\n",
    "        for i in range(index - 1, -1, -1):\n",
    "            if df.loc[i, 'action'] != 'T':\n",
    "                continue\n",
    "            if df.loc[i, 'price'] != df.loc[index, 'price']:\n",
    "                if df.loc[index, 'price'] > df.loc[i, 'price']:\n",
    "                    return 'B'  \n",
    "                else:\n",
    "                    return 'A'  \n",
    "        return 'B'  # Default to 'B' if no differing trade price is found\n",
    "\n",
    "    # Apply the rules to update the 'side' column\n",
    "    for index, row in df.iterrows():\n",
    "        if row['side'] == 'N':\n",
    "            if row['price'] > row['mid_px_00_minus']:\n",
    "                df.at[index, 'side'] = 'B'\n",
    "            elif row['price'] < row['mid_px_00_minus']:\n",
    "                df.at[index, 'side'] = 'A'\n",
    "            else:\n",
    "                df.at[index, 'side'] = apply_tick_test(df, index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_time_blocks(df, ts_event_col='ts_event', start_time='09:30:00', end_time='16:00:00', num_blocks=6):\n",
    "    \"\"\"\n",
    "    Adds a 'block' column to the dataframe, dividing the time between start_time and end_time\n",
    "    into roughly equal periods.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "        ts_event_col (str): The name of the timestamp column (default: 'ts_event').\n",
    "        start_time (str): The start time of the period (default: '09:30:00').\n",
    "        end_time (str): The end time of the period (default: '16:00:00').\n",
    "        num_blocks (int): The number of blocks to divide the time into (default: 6).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with the added 'block' column.\n",
    "    \"\"\"\n",
    "    # Ensure the timestamp column is timezone-aware\n",
    "    if not pd.api.types.is_datetime64tz_dtype(df[ts_event_col]):\n",
    "        raise ValueError(f\"The column '{ts_event_col}' must be timezone-aware.\")\n",
    "\n",
    "    # Get the timezone of the ts_event column\n",
    "    tz = df[ts_event_col].dt.tz\n",
    "\n",
    "    start_time = pd.to_datetime(start_time).time()\n",
    "    end_time = pd.to_datetime(end_time).time()\n",
    "    df['date'] = df[ts_event_col].dt.date\n",
    "    df['start_datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + start_time.strftime('%H:%M:%S')).dt.tz_localize(tz)\n",
    "    df['end_datetime'] = pd.to_datetime(df['date'].astype(str) + ' ' + end_time.strftime('%H:%M:%S')).dt.tz_localize(tz)\n",
    "\n",
    "    # Calculate the total time range in seconds\n",
    "    total_seconds = (df['end_datetime'] - df['start_datetime']).dt.total_seconds()\n",
    "\n",
    "    # Divide the time range into equal blocks\n",
    "    df['block'] = pd.cut(\n",
    "        (df[ts_event_col] - df['start_datetime']).dt.total_seconds(),\n",
    "        bins=num_blocks,\n",
    "        labels=range(1, num_blocks + 1)\n",
    "    )\n",
    "\n",
    "    df.drop(columns=['date', 'start_datetime', 'end_datetime'], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "execution_count":15,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Y3OHB9nfkpSFY77XsiZuVE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Functions to define order-level features used in the paper\n",
    "\n",
    "def bbo_moving_trade(df):\n",
    "    df['bbo_moving_trade'] = np.where(((df['action'] == 'T') & (df['side'] == 'A') & (df['size'] >= df['bid_sz_00_minus'])) |\n",
    "                                      ((df['action'] == 'T') & (df['side'] == 'B') & (df['size'] >= df['ask_sz_00_minus'])),\n",
    "                                      1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_moving_trade(df):\n",
    "    df['non_bbo_moving_trade'] = np.where(((df['action'] == 'T') & (df['side'] == 'A') & (df['size'] < df['bid_sz_00_minus'])) |\n",
    "                                          ((df['action'] == 'T') & (df['side'] == 'B') & (df['size'] < df['ask_sz_00_minus'])),\n",
    "                                          1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bbo_improving_limit(df):\n",
    "    df['bbo_improving_limit'] = np.where(((df['action'] == 'A') & (df['side'] == 'A') & (df['price'] < df['ask_px_00_minus'])) |\n",
    "                                         ((df['action'] == 'A') & (df['side'] == 'B') & (df['price'] > df['bid_px_00_minus'])),\n",
    "                                         1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bbo_worsening_cancel(df):\n",
    "    df['bbo_worsening_cancel'] = np.where(((df['action'] == 'C') & (df['side'] == 'A') & (df['price'] <= df['ask_px_00_minus']) &\n",
    "                                                (df['size'] >= df['ask_sz_00_minus'])) |\n",
    "                                          ((df['action'] == 'C') & (df['side'] == 'B') & (df['price'] >= df['bid_px_00_minus']) &\n",
    "                                                (df['size'] >= df['bid_sz_00_minus'])),\n",
    "                                          1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bbo_depth_add_limit(df):\n",
    "    df['bbo_depth_add_limit'] = np.where(((df['action'] == 'A') & (df['side'] == 'A') & (df['price'] == df['ask_px_00_minus'])) |\n",
    "                                         ((df['action'] == 'A') & (df['side'] == 'B') & (df['price'] == df['bid_px_00_minus'])),\n",
    "                                          1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bbo_depth_remove_cancel(df):\n",
    "    df['bbo_depth_remove_cancel'] = np.where(((df['action'] == 'C') & (df['side'] == 'A') & (df['price'] == df['ask_px_00_minus']) &\n",
    "                                                (df['size'] < df['ask_sz_00_minus'])) |\n",
    "                                             ((df['action'] == 'C') & (df['side'] == 'B') & (df['price'] == df['bid_px_00_minus']) &\n",
    "                                                (df['size'] < df['bid_sz_00_minus'])),\n",
    "                                             1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_depth_add_limit(df):\n",
    "    df['non_bbo_depth_add_limit'] = np.where(((df['action'] == 'A') & (df['side'] == 'A') & (df['price'] > df['ask_px_00_minus']) &\n",
    "                                              (df['price'] < df['ask_px_05_minus'])) |\n",
    "                                             ((df['action'] == 'A') & (df['side'] == 'B') & (df['price'] < df['bid_px_00_minus']) &\n",
    "                                              (df['price'] > df['bid_px_05_minus'])),\n",
    "                                             1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_depth_remove_cancel(df):\n",
    "    df['non_bbo_depth_remove_cancel'] = np.where(((df['action'] == 'C') & (df['side'] == 'A') & (df['price'] > df['ask_px_00_minus']) &\n",
    "                                                  (df['price'] < df['ask_px_05_minus'])) |\n",
    "                                                 ((df['action'] == 'C') & (df['side'] == 'B') & (df['price'] < df['bid_px_00_minus']) &\n",
    "                                                  (df['price'] > df['bid_px_05_minus'])),\n",
    "                                                 1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_deep_depth_add_limit(df):\n",
    "    df['non_bbo_deep_depth_add_limit'] = np.where(((df['action'] == 'A') & (df['side'] == 'A') & (df['price'] >= df['ask_px_05_minus'])) |\n",
    "                                                  ((df['action'] == 'A') & (df['side'] == 'B') & (df['price'] < df['bid_px_05_minus'])),\n",
    "                                                  1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_deep_depth_remove_cancel(df):\n",
    "    df['non_bbo_deep_depth_remove_cancel'] = np.where(((df['action'] == 'C') & (df['side'] == 'A') & (df['price'] > df['ask_px_05_minus'])) |\n",
    "                                                 ((df['action'] == 'C') & (df['side'] == 'B') & (df['price'] < df['bid_px_05_minus'])),\n",
    "                                                 1, 0)\n",
    "    return df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def lagged_ob_features(df):\n",
    "    ob_features = ['bbo_moving_trade', 'non_bbo_moving_trade', 'bbo_improving_limit',\n",
    "                   'bbo_worsening_cancel', 'bbo_depth_add_limit', 'bbo_depth_remove_cancel',\n",
    "                   'non_bbo_depth_add_limit', 'non_bbo_depth_remove_cancel', \n",
    "                   'non_bbo_deep_depth_add_limit', 'non_bbo_deep_depth_remove_cancel']\n",
    "    \n",
    "    new_cols = {}\n",
    "    for feat in ob_features:\n",
    "        for r in range(1, 6):\n",
    "            col_name = f'{feat}_{r}'\n",
    "            new_cols[col_name] = df[feat].shift(1).rolling(r).sum().fillna(0)\n",
    "    \n",
    "    new_cols_df = pd.DataFrame(new_cols, index=df.index)\n",
    "    df = pd.concat([df, new_cols_df], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_ob_features(df):\n",
    "    df = bbo_moving_trade(df)\n",
    "    df = non_bbo_moving_trade(df)\n",
    "    df = bbo_improving_limit(df)\n",
    "    df = bbo_worsening_cancel(df)\n",
    "    df = bbo_depth_add_limit(df)\n",
    "    df = bbo_depth_remove_cancel(df)\n",
    "    df = non_bbo_depth_add_limit(df)\n",
    "    df = non_bbo_depth_remove_cancel(df)\n",
    "    df = non_bbo_deep_depth_add_limit(df)\n",
    "    df = non_bbo_deep_depth_remove_cancel(df)\n",
    "    df = lagged_ob_features(df)\n",
    "\n",
    "    return df"
   ],
   "execution_count":16,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"ykuQ0F20gRZdPubeT2Joek",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "# Functions to define state-level features used in the paper\n",
    "\n",
    "def add_mid_px(df):\n",
    "    df['mid_px_00'] = (df['bid_px_00'] + df['ask_px_00']) \/ 2\n",
    "    return df\n",
    "\n",
    "\n",
    "def spread(df):\n",
    "    df['spread'] = (df['ask_px_00'] - df['bid_px_00']) \/ df['mid_px_00']\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_missing(df):\n",
    "    lvls = ['00', '01', '02', '03', '04', \n",
    "            '05', '06', '07', '08', '09']\n",
    "    \n",
    "    bid_fillers = [f'bid_sz_{i}' for i in lvls]\n",
    "    ask_fillers = [f'ask_sz_{i}' for i in lvls]\n",
    "    fillers = bid_fillers + ask_fillers\n",
    "\n",
    "    df[fillers] = df[fillers].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def bbo_depth_imbalance(df):\n",
    "    df['bbo_depth_imbalance'] = df['bid_sz_00'] - df['ask_sz_00']\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_depth_imbalance(df):\n",
    "    df['non_bbo_depth_imbalance'] = (df['bid_sz_00'] + df['bid_sz_01'] + df['bid_sz_02'] + df['bid_sz_03'] + df['bid_sz_04']) - \\\n",
    "                                    (df['ask_sz_00'] + df['ask_sz_01'] + df['ask_sz_02'] + df['ask_sz_03'] + df['ask_sz_04'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def non_bbo_deep_depth_imbalance(df):\n",
    "    df['non_bbo_deep_depth_imbalance'] = (df['bid_sz_05'] + df['bid_sz_06'] + df['bid_sz_07'] + df['bid_sz_08'] + df['bid_sz_09']) - \\\n",
    "                                         (df['ask_sz_05'] + df['ask_sz_06'] + df['ask_sz_07'] + df['ask_sz_08'] + df['ask_sz_09'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def bbo_queue_length_immbalance(df):\n",
    "    df['bbo_queue_length_immbalance'] = df['bid_ct_00'] - df['ask_ct_00']\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_state_features(df):\n",
    "    df = add_mid_px(df)\n",
    "    df = spread(df)\n",
    "    df = fill_missing(df)\n",
    "    df = bbo_depth_imbalance(df)\n",
    "    df = non_bbo_depth_imbalance(df)\n",
    "    df = non_bbo_deep_depth_imbalance(df)\n",
    "    df = bbo_queue_length_immbalance(df)\n",
    "    return df"
   ],
   "execution_count":17,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"3Y3fQxK8wJusMkn5BB9o05",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def add_px_impact(df):\n",
    "    for i in [0, 1, 5, 10, 20]:\n",
    "        if i == 0:\n",
    "            df[f'px_imp_{i}'] = np.log(df['mid_px_00_plus'] \/ df['mid_px_00_minus'])\n",
    "        else:\n",
    "            df[f'px_imp_{i}'] = np.log(df['mid_px_00_plus'].shift(-i) \/ df['mid_px_00_minus'])\n",
    "    \n",
    "    return df"
   ],
   "execution_count":18,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"KXwUd0mLiUh42mSEgufB5t",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def process_day_of_data(ticker, year, month, day, exchange):\n",
    "    ob = import_raw_mbo_data(ticker, year, month, day, exchange)\n",
    "    mbp = import_raw_mbp_data(ticker, year, month, day, exchange)\n",
    "\n",
    "    ob = filter_order_actions(ob)\n",
    "    ob = combine_T_rows(ob)\n",
    "    mbp = add_state_features(mbp)\n",
    "\n",
    "    df = create_p_plus_minus(ob, mbp)\n",
    "    df = assign_side(df)\n",
    "    df = calculate_time_blocks(df)\n",
    "    df = pd.get_dummies(df, columns=['block'], prefix='block', drop_first=True)\n",
    "    df['bid'] = np.where(df['side'] == 'B', 1, 0)\n",
    "    df = add_ob_features(df)\n",
    "    df = add_px_impact(df)\n",
    "\n",
    "    df = df.dropna(subset=['px_imp_0', 'px_imp_1', 'px_imp_5', 'px_imp_10', 'px_imp_20'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_batch_of_data(tickers, years, months, days, exchanges):\n",
    "    df_list = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        for year in years:\n",
    "            for month in months:\n",
    "                for day in days:\n",
    "                    df0 = process_day_of_data(ticker, year, month, day, exchanges[0])\n",
    "                    df1 = process_day_of_data(ticker, year, month, day, exchanges[1])\n",
    "                    df2 = pd.concat([df0, df1])\n",
    "                    df2.to_pickle(f'\/data\/workspace_files\/{ticker}-{year}{month}{day}-df.pkl')\n",
    "                    # df_list.append(df2)\n",
    "        \n",
    "    # dfs = pd.concat(df_list, ignore_index=True)\n",
    "    # get_data_statistics(dfs)\n",
    "\n",
    "    # return dfs"
   ],
   "execution_count":19,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"Yi2yuZ7aP8h3W9oBrt7dQ0",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def get_data_statics(df):\n",
    "    # List of event indicator columns\n",
    "    list1 = ['bbo_moving_trade', 'non_bbo_moving_trade', 'bbo_improving_limit', \n",
    "             'bbo_worsening_cancel', 'bbo_depth_add_limit', 'bbo_depth_remove_cancel', \n",
    "             'non_bbo_depth_add_limit', 'non_bbo_depth_remove_cancel',  \n",
    "             'non_bbo_deep_depth_add_limit', 'non_bbo_deep_depth_remove_cancel']\n",
    "    \n",
    "\n",
    "    data = []\n",
    "    for feat in list1:\n",
    "        # Filter rows where the event occurred (assuming an indicator value of 1)\n",
    "        df_event = df[df[feat] == 1]\n",
    "        num = len(df_event)\n",
    "        median_size = df_event['size'].median()\n",
    "        max_size = df_event['size'].max()\n",
    "        avg_price = df_event['price'].mean()\n",
    "        p_imp0 = np.abs(df_event['px_imp_0']).mean()\n",
    "        p_imp20 = np.abs(df_event['px_imp_20']).mean()\n",
    "        \n",
    "        \n",
    "        data.append({\n",
    "            'Event': feat,\n",
    "            'Count': num,\n",
    "            'Median Size': median_size,\n",
    "            'Max Size': max_size,\n",
    "            'Avg Price': avg_price,\n",
    "            'Avg Price Impact': p_imp0,\n",
    "            'Avg Permanent Price Impact': p_imp20\n",
    "        })\n",
    "\n",
    "    event_metrics_df = pd.DataFrame(data)\n",
    "    \n",
    "    avg_spread = df['spread_minus'].mean()\n",
    "    spread_df = pd.DataFrame({\n",
    "        'Metric': ['Average Spread'],\n",
    "        'Value': [avg_spread]\n",
    "    })\n",
    "    \n",
    "    return event_metrics_df, spread_df"
   ],
   "execution_count":20,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"78CnPy32ErVX7SmEYTkuND",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Get Train and Test Data"
   ],
   "attachments":{},
   "metadata":{
    "datalore":{
     "node_id":"BH7Mdk0CknyJp1GTpbp65I",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "tickers = ['JPM', 'HPE', 'LNT', 'LULU', 'CCL', 'BBIO', 'NTNX', 'KO', 'NFE', 'MVST']\n",
    "years = ['2024']\n",
    "months = ['10', '11', '12']\n",
    "\n",
    "train_list_oct = [\n",
    "    '01', '02', '03', '04',  \n",
    "    '07', '08', '09', '10', '11',\n",
    "    '14', '15', '16', '17', '18'  \n",
    "]\n",
    "\n",
    "\n",
    "val_list_oct = [ \n",
    "    '21', '22', '23', '24', '25', \n",
    "    '28', '29', '30', '31'  \n",
    "]\n",
    "\n",
    "\n",
    "train_list_nov = [\n",
    "    '01',                     \n",
    "    '04', '05', '06', '07', '08',  \n",
    "    '11', '12', '13', '14', '15'\n",
    "]\n",
    "\n",
    "\n",
    "val_list_nov = [\n",
    "    '18', '19', '20', '21', '22',  \n",
    "    '25', '26', '27',       '29'   \n",
    "]\n",
    "\n",
    "\n",
    "test_list_dec = [\n",
    "    '02', '03', '04', '05', '06',  \n",
    "    '09', '10', '11', '12', '13'  \n",
    "]\n",
    "\n",
    "exchanges = ['xnas', 'nyse']"
   ],
   "execution_count":8,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"sFHxgqChaHkLXIh0TZxqEk",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "process_batch_of_data(tickers, years, ['11'], train_list_nov, exchanges)"
   ],
   "execution_count":0,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"241ozFy9mmtpC7rXSHeg2l",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "process_batch_of_data(tickers, years, ['11'], val_list_nov, exchanges)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"V8y0r6yf0ITc95IQhqAVOa",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "process_batch_of_data(tickers, years, ['12'], test_list_dec, exchanges)"
   ],
   "execution_count":null,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"cz2LP63YmAw5CSDo3gzLXS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "df = pd.read_parquet('\/data\/workspace_files\/train_oct\/first_week_df.parquet')"
   ],
   "execution_count":5,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"q9v2ovWaHbdA0PCTFLesZ3",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "get_data_statics(df)"
   ],
   "execution_count":11,
   "outputs":[],
   "metadata":{
    "datalore":{
     "node_id":"ITiypchJFiJ06utICC3qxK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"PyEnv311",
   "packages":[],
   "report_row_ids":[],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}
